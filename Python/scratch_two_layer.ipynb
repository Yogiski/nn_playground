{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Layer NN Digit Classifier Using MNIST \n",
    "\n",
    "MNIST is a dataset of low res images of hand written digits.\n",
    "In this notebook I will explore the math and implement from scratch in python different nueral network structures and compare their performance.\n",
    " \n",
    "Data Structure and Notation:\n",
    "* 28 x 28 pixel images, 784 pixels\n",
    "* $m$ training images\n",
    "* $X$ is a matrix with dimensions $m$ by 784 \n",
    "    * $X^{T}$ is used for input into nueral network model\n",
    "\n",
    " ## Basic NN Structure\n",
    "input layer: 784 nodes  \n",
    "* 28 by 28 images flattened into a vector  \n",
    "\n",
    "hidden layer: h nodes (h in image below) \n",
    "* This parameter should be optimized based on performace. \n",
    "\n",
    "output layer: 10 nodes (o in images below)\n",
    "* 10 classes or digits, so 10 nodes \n",
    "\n",
    "Note: In images below true matrix size is not shown. Dimensions are noted by variables next to each axis.\n",
    "\n",
    "## Forward Propagation\n",
    "Forward propagation is the process of transforming our input data into our output data.   \n",
    "Forward propagation for our two layer neural network is imaged below.  \n",
    "\n",
    "![forward_prop](../data/imgs/2layer_forward_prop.svg)\n",
    "\n",
    "The input layer is simply a matrix of our raw training data.  \n",
    "Input Layer Definition:\n",
    "* $A^{[0]} = X\\space\\space\\text{with dimensions } 784\\space \\text{x} \\space m$\n",
    "\n",
    "The hidden layer values are obtained by a linear transformation of the input layer followed by a non-linear transformation or activation of the transformed input data.  \n",
    "We are using the rectified linear unit or ReLU as the activation function for our hidden layer. \n",
    "\n",
    "$\\text{RelU}(x) = \\{x \\hspace{0.5em} \\text{if} \\hspace{0.5em} x > 0, \\hspace{1em} 0 \\hspace{0.5em} \\text{if} \\hspace{0.5em} \\le 0\\}$ \n",
    "\n",
    "Hidden Layer Definition:\n",
    "* $Z^{[1]} = w^{[1]} \\cdot A^{[0]} + b^{[1]}$ (Unactivated: linear transformation of input layer)\n",
    "* $A^{[1]} = g(Z^{[1]}) = \\text{ReLu}(Z^{[1]})$ (Activated: ReLU transformation of linear transformation)\n",
    "\n",
    "The output layer works similarly to the hidden layer but with an output specific activation function. \\\n",
    "The hidden layer is linearally transformed and this time we use a softmax as an activation function to convert our transformed values into probabilities.   \n",
    "The softmax essentially calculates the contribution of each not to the total output vector.\n",
    "\n",
    "$\\text{softmax}(x) = \\dfrac{e^{x_{i}}}{\\sum_{i=1}^{k} e^{x_{i}}}$\n",
    "\n",
    "* $Z^{[2]} = w^{[2]} \\cdot A^{[2]} + b^{[2]}$ (Unactivated: linear tranformation of hidden layer)\n",
    "* $A^{[2]} = \\text{softmax}(Z^{[2]})$ (Activated: probabilities of input being a given class)\n",
    "\n",
    "## Back Propagation\n",
    "Back Propagation is the process we use to learn the weights and biases in the forward propagation process.  \n",
    "It works by finding the error in the weights, biases, and predictions working backward from the output layer. \n",
    "This process for our two layer neural network is imaged below.  \n",
    "\n",
    "![back_prop](../data/imgs/2layer_back_prop.svg)\n",
    "\n",
    "First step is to find the error of the output layer by subtracting the classes from our output layer matrix.  \n",
    "Y is the digit pictured in each image in the training set. Y is a one-hot encoded 10 by m matrix. \n",
    "\n",
    "* $dZ^{[2]} = A^{[2]} - Y$ (error of predictions)\n",
    "\n",
    "The next step is to find the error associated with weights and biases used in the linear transformation in the second layer.\n",
    "\n",
    "* $dw^{[2]} = \\frac{1}{m} dZ^{[2]} \\cdot A^{[1]T}$ (contributions of layer 2 weights to prediction error)\n",
    "* $db^{[2]} = \\frac{1}{m} \\sum dZ^{[2]}$ (contributions of layer 2 biases to prediction error)\n",
    "\n",
    "Repeat this process to find error of weights and biases in the first layer. Must undo activation function. \n",
    "$g`$ is the derivative of the activation function\n",
    "* $dZ^{[1]} = w^{[2]T} dZ^{[2]} \\cdot g`(Z^{[1]})$ (error of Hidden Layer transformation)\n",
    "* $dw^{[1]} = \\frac{1}{m} dZ^{[1]} X^T$ (contribution of weights to layer 2 error)\n",
    "* $db^{[1]} = \\frac{1}{m} \\sum dZ^{[1]}$ (contribution of biases to layer 2 error)\n",
    "\n",
    "## Weight and Bias Update\n",
    "Weights and biases are then updated after calculating their errors in backpropagation. \\\n",
    "The new values are generated by subtracting the error multiplied by a learning rate parameter. \\\n",
    "The learning rate $\\alpha$ is a user defined constant that modifies how much the weights and biases are changed in each cycle\n",
    "\n",
    "* $w^{[1_{}]}_{new} = w^{[1]} - \\alpha dw^{[1]}$\n",
    "* $b^{[1]}_{new} = b^{[1]} - \\alpha db^{[1]}$\n",
    "* $w^{[2]}_{new} = w^{[2]} - \\alpha dw^{[1]}$\n",
    "* $b^{[2]}_{new} = b^{[2]} - \\alpha db^{[2]}$\n",
    "\n",
    "This process of using forward propagation to make predictions follow by backward propagation to find the error and \"learn\" new weights \\\n",
    "is repeats till our predictions reach reach some sort of convergence. This training process in known as gradient descent. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Code ###\n",
    "Below I've programmed some functions and classes to make a two layer neural network from scratch. \\\n",
    "I've chosen to make general classes for a layer and a Two Layer nn so I can easily change the size of the layers and other parameters for optimization.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "### ACTIVATION FUNCTIONS ####\n",
    "def ReLU(vec: np.array):\n",
    "    return np.maximum(vec, 0)\n",
    "\n",
    "def deriv_ReLU(vec: np.array):\n",
    "    return vec > 0\n",
    "\n",
    "def softmax(vec: np.array):\n",
    "    exp = np.exp(vec - np.max(vec))\n",
    "    probs = exp / exp.sum(axis = 0)\n",
    "    return probs\n",
    "\n",
    "#### LOSS FUNCTIONS ####\n",
    "\n",
    "def mse(pred:np.array, one_hot_labels: np.array):\n",
    "    return np.mean((pred - one_hot_labels) ** 2)\n",
    "\n",
    "def mae(pred:np.array, one_hot_labels: np.array):\n",
    "    return np.mean(np.abs(pred - one_hot_labels))\n",
    "\n",
    "#### Utility Functions ####\n",
    "def get_predictions(nn_out):\n",
    "    return np.argmax(nn_out, 0)\n",
    "\n",
    "def get_accuracy(pred, labels):\n",
    "    return np.sum(pred == labels) / labels.size\n",
    "\n",
    "def one_hot(vec: np.array):\n",
    "    one_hot_vec = np.zeros((vec.size, vec.max() + 1))\n",
    "    one_hot_vec[np.arange(vec.size), vec] = 1\n",
    "    return one_hot_vec.T\n",
    "\n",
    "\n",
    "##### NN CLASSES #######\n",
    "\n",
    "class Layer(object):\n",
    "\n",
    "\n",
    "    def __init__(self, layer_id, input_size, node_count, activation_func):\n",
    "\n",
    "        # Layer metadata\n",
    "        self.layer_id = layer_id\n",
    "        self.node_count = node_count\n",
    "        self.activation_func = activation_func\n",
    "        # random init of weights and biases\n",
    "        self.weights = np.random.rand(node_count, input_size) - 0.5\n",
    "        self.biases = np.random.rand(node_count, 1) - 0.5\n",
    "    \n",
    "    def transform(self, input_dat: np.array):\n",
    "\n",
    "        lin_transform = self.weights.dot(input_dat) + self.biases\n",
    "        self.unactivated_node_values = lin_transform\n",
    "        node_values = self.activation_func(lin_transform) \n",
    "        self.node_values = node_values\n",
    "\n",
    "        return node_values\n",
    "\n",
    "    def update_weights_n_biases(self, weight_error, bias_error, learning_rate):\n",
    "\n",
    "        self.weights -= learning_rate * weight_error\n",
    "        self.biases -= learning_rate * bias_error.reshape(self.node_count, 1)\n",
    "\n",
    "\n",
    "class TwoLayerNN(object):\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            train_dat: np.array,\n",
    "            train_labels: np.array,\n",
    "            test_dat: np.array,\n",
    "            test_labels: np.array,\n",
    "            hidden_nodes: int,\n",
    "            output_nodes: int,\n",
    "            learning_rate: float,\n",
    "        ):\n",
    "\n",
    "        self.train_dat = train_dat / 255 # divide to make values between 0 and 1\n",
    "        self.train_labels = train_labels\n",
    "        self.one_hot_train_labels = one_hot(train_labels)\n",
    "        self.test_dat = test_dat / 255 # divide to make values between 0 and 1\n",
    "        self.test_labels = test_labels\n",
    "        self.one_hot_test_labels = one_hot(test_labels)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_layer = Layer(\"hidden\", train_dat.shape[0], hidden_nodes, activation_func = ReLU)\n",
    "        self.output_layer = Layer(\"out\", hidden_nodes, output_nodes, activation_func = softmax)\n",
    "\n",
    "    def forward_prop(self, dataset = \"train\"):\n",
    "\n",
    "        if dataset == \"train\":\n",
    "            data = self.train_dat\n",
    "        elif dataset == \"test\":\n",
    "            data = self.test_dat\n",
    "\n",
    "        hidden_transformed = self.hidden_layer.transform(data)\n",
    "        out = self.output_layer.transform(hidden_transformed)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward_prop(self, out):\n",
    "\n",
    "        m = self.train_dat.shape[1]\n",
    "        # prediction error\n",
    "        dZ_o = out - self.one_hot_train_labels\n",
    "        dw_o = (1 / m) * dZ_o.dot(self.hidden_layer.node_values.T) \n",
    "        db_o =  (1 / m) * np.sum(dZ_o, 1)\n",
    "        # hidden layer error\n",
    "        dZ_h =  self.output_layer.weights.T.dot(dZ_o) * deriv_ReLU(self.hidden_layer.unactivated_node_values)\n",
    "        dw_h = (1 / m) * dZ_h.dot(self.train_dat.T)\n",
    "        db_h =  (1 / m) * np.sum(dZ_h, 1)\n",
    "        # update weights and biases from error components\n",
    "        self.hidden_layer.update_weights_n_biases(dw_h, db_h, self.learning_rate)\n",
    "        self.output_layer.update_weights_n_biases(dw_o, db_o, self.learning_rate)\n",
    "\n",
    "    def gradient_descent(self, iterations):\n",
    "\n",
    "        print(f\"Running model with {self.hidden_layer.node_count} hidden layer nodes\")\n",
    "\n",
    "        for i in range(iterations):\n",
    "            pred = self.forward_prop(dataset = \"train\")\n",
    "            self.backward_prop(pred)\n",
    "            if (i + 1) % 1000 == 0:\n",
    "                dig_pred = get_predictions(pred)\n",
    "                acc = get_accuracy(dig_pred, self.train_labels)\n",
    "                print(f\"training accuracy @ iter {i+1} = {acc}\")\n",
    "    \n",
    "    def test_accuracy(self):\n",
    "\n",
    "        pred = self.forward_prop(dataset = \"test\")\n",
    "        dig_pred = get_predictions(pred)\n",
    "        acc = get_accuracy(dig_pred, self.test_labels)\n",
    "        print(f\"Testing accuracy = {acc}\")\n",
    "\n",
    "        return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data Visualizing an Image\n",
    "In the code below I'm loading in the data and formatting to instantiate my nueral network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2049 60000\n",
      "2051 60000 28 28\n",
      "[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   3.  18.\n",
      "  18.  18. 126. 136. 175.  26. 166. 255. 247. 127.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.  30.  36.  94. 154. 170. 253.\n",
      " 253. 253. 253. 253. 225. 172. 253. 242. 195.  64.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.  49. 238. 253. 253. 253. 253. 253.\n",
      " 253. 253. 253. 251.  93.  82.  82.  56.  39.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.  18. 219. 253. 253. 253. 253. 253.\n",
      " 198. 182. 247. 241.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.  80. 156. 107. 253. 253. 205.\n",
      "  11.   0.  43. 154.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.  14.   1. 154. 253.  90.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 139. 253. 190.\n",
      "   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  11. 190. 253.\n",
      "  70.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  35. 241.\n",
      " 225. 160. 108.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  81.\n",
      " 240. 253. 253. 119.  25.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  45. 186. 253. 253. 150.  27.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.  16.  93. 252. 253. 187.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0. 249. 253. 249.  64.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  46. 130. 183. 253. 253. 207.   2.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  39. 148.\n",
      " 229. 253. 253. 253. 250. 182.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  24. 114. 221. 253.\n",
      " 253. 253. 253. 201.  78.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.  23.  66. 213. 253. 253. 253.\n",
      " 253. 198.  81.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  18. 171. 219. 253. 253. 253. 253. 195.\n",
      "  80.   9.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.  55. 172. 226. 253. 253. 253. 253. 244. 133.  11.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0. 136. 253. 253. 253. 212. 135. 132.  16.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "2049 10000\n",
      "2051 10000 28 28\n",
      "[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  84. 185. 159. 151.  60.  36.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0. 222. 254. 254. 254. 254. 241. 198. 198.\n",
      " 198. 198. 198. 198. 198. 198. 170.  52.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  67. 114.  72. 114. 163. 227. 254. 225.\n",
      " 254. 254. 254. 250. 229. 254. 254. 140.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  17.  66.  14.\n",
      "  67.  67.  67.  59.  21. 236. 254. 106.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.  83. 253. 209.  18.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.  22. 233. 255.  83.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0. 129. 254. 238.  44.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.  59. 249. 254.  62.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0. 133. 254. 187.   5.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   9. 205. 248.  58.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0. 126. 254. 182.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  75. 251. 240.  57.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  19.\n",
      " 221. 254. 166.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   3. 203.\n",
      " 254. 219.  35.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  38. 254.\n",
      " 254.  77.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  31. 224. 254.\n",
      " 115.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 133. 254. 254.\n",
      "  52.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  61. 242. 254. 254.\n",
      "  52.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 121. 254. 254. 219.\n",
      "  40.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 121. 254. 207.  18.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11ffc7710>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datpath = \"../../data/mnist\"\n",
    "\n",
    "def read_mnist(im_path, label_path):\n",
    "\n",
    "    # open training binary training data \n",
    "    with open(label_path, mode = \"rb\") as f:\n",
    "        # first 8 bytes hold label metadata\n",
    "        magic, items, = struct.unpack(\">II\", f.read(8)) \n",
    "        print(magic, items)\n",
    "        buf = f.read()\n",
    "        labels = np.frombuffer(buf, dtype = np.uint8)\n",
    "        f.close()\n",
    "    \n",
    "    with open(im_path, mode = \"rb\") as f:\n",
    "        # first 16 bytes hold image metadata\n",
    "        magic, items, dim1, dim2 = struct.unpack(\">IIII\", f.read(16)) \n",
    "        print(magic, items, dim1, dim2)\n",
    "        buf = f.read()\n",
    "        imgs = np.frombuffer(buf, dtype = np.uint8).astype(np.float32)\n",
    "        # reshape data into matrix of flattened images\n",
    "        imgs = imgs.reshape(items, dim1 * dim2)\n",
    "        f.close()\n",
    "\n",
    "    return imgs, labels \n",
    "\n",
    "\n",
    "# read in training \n",
    "train_dat, train_labels = read_mnist(f\"{datpath}/train-images.idx3-ubyte\", f\"{datpath}/train-labels.idx1-ubyte\")\n",
    "test_dat, test_labels = read_mnist(f\"{datpath}/t10k-images.idx3-ubyte\", f\"{datpath}/t10k-labels.idx1-ubyte\")\n",
    "\n",
    "# show example of image in dataset\n",
    "# data is in flattened vector format, must be reshaped to 2d image to be viewed\n",
    "im_num = 546 \n",
    "image = train_dat[im_num, :].reshape(28, 28)\n",
    "print(train_labels[im_num])\n",
    "plt.imshow(image, cmap = plt.cm.gray_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Neural Network\n",
    "Below I train the two layer neural network with a 1 hidden layer with 10 nodes, a learning rate of 0.1 and 1000 training iterations.  \n",
    "This initial model ends up with testing and training accuracy around mid 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100 accuracy = 0.5579166666666666\n",
      "iter: 200 accuracy = 0.6876\n",
      "iter: 300 accuracy = 0.74285\n",
      "iter: 400 accuracy = 0.7777\n",
      "iter: 500 accuracy = 0.8039166666666666\n",
      "iter: 600 accuracy = 0.8230666666666666\n",
      "iter: 700 accuracy = 0.8373333333333334\n",
      "iter: 800 accuracy = 0.8479\n",
      "iter: 900 accuracy = 0.8557833333333333\n",
      "iter: 1000 accuracy = 0.8629833333333333\n",
      "Testing accuracy = 0.8638\n"
     ]
    }
   ],
   "source": [
    "# init NN object\n",
    "nn = TwoLayerNN(train_dat.T, train_labels, test_dat.T, test_labels, hidden_nodes = 10, output_nodes = 10, learning_rate = 0.10)\n",
    "nn.gradient_descent(iterations = 1000)\n",
    "nn.test_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layer Node Count Optimization\n",
    "Below I will attempt to optimize the two layer neural network with the following constraints.\n",
    "* learning rate = 0.1\n",
    "* only optimizing the number of nodes in the hidden layer \n",
    "* only using 100 nodes maximun to save training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model with 10 hidden layer nodes\n",
      "training accuracy @ iter 1000 = 0.8772333333333333\n",
      "Testing accuracy = 0.8813\n",
      "Running model with 100 hidden layer nodes\n",
      "training accuracy @ iter 1000 = 0.9278\n",
      "Testing accuracy = 0.9267\n",
      "Running model with 55 hidden layer nodes\n",
      "training accuracy @ iter 1000 = 0.9131333333333334\n",
      "Testing accuracy = 0.9141\n",
      "best model: 100 nodes \n",
      "Testing accuracy = 0.9267\n",
      "Running model with 77 hidden layer nodes\n",
      "training accuracy @ iter 1000 = 0.9190166666666667\n",
      "Testing accuracy = 0.9171\n",
      "best model: 100 nodes \n",
      "Testing accuracy = 0.9267\n",
      "Running model with 88 hidden layer nodes\n",
      "training accuracy @ iter 1000 = 0.9188333333333333\n",
      "Testing accuracy = 0.92\n",
      "best model: 100 nodes \n",
      "Testing accuracy = 0.9267\n",
      "Running model with 94 hidden layer nodes\n",
      "training accuracy @ iter 1000 = 0.9245333333333333\n",
      "Testing accuracy = 0.92\n",
      "best model: 100 nodes \n",
      "Testing accuracy = 0.9267\n",
      "Running model with 94 hidden layer nodes\n",
      "training accuracy @ iter 1000 = 0.92155\n",
      "Testing accuracy = 0.9154\n",
      "best model: 100 nodes \n",
      "Testing accuracy = 0.9267\n",
      "Running model with 94 hidden layer nodes\n",
      "training accuracy @ iter 1000 = 0.9238166666666666\n",
      "Testing accuracy = 0.9207\n",
      "best model: 100 nodes \n",
      "Testing accuracy = 0.9267\n",
      "Running model with 97 hidden layer nodes\n",
      "training accuracy @ iter 1000 = 0.9255666666666666\n",
      "Testing accuracy = 0.9256\n",
      "best model: 100 nodes \n",
      "Testing accuracy = 0.9267\n",
      "Running model with 98 hidden layer nodes\n",
      "training accuracy @ iter 1000 = 0.9239833333333334\n",
      "Testing accuracy = 0.9203\n",
      "best model: 100 nodes \n",
      "Testing accuracy = 0.9267\n"
     ]
    }
   ],
   "source": [
    "#set random state for cross model comparison\n",
    "np.random.RandomState(seed = 7734)\n",
    "\n",
    "min_nodes = 10\n",
    "max_nodes = 100\n",
    "\n",
    "def split(min_n, max_n):\n",
    "    return int((min_n + max_n) / 2)\n",
    "\n",
    "def run_model(node_count):\n",
    "\n",
    "    nodes = split(min_nodes, max_nodes)\n",
    "    nn = TwoLayerNN(train_dat.T, train_labels, test_dat.T, test_labels, hidden_nodes = node_count, output_nodes = 10, learning_rate = 0.10)\n",
    "    nn.gradient_descent(iterations = 1000)\n",
    "    acc = nn.test_accuracy()\n",
    "\n",
    "    return acc, nn\n",
    "\n",
    "min_acc, min_mod = run_model(min_nodes)\n",
    "max_acc, max_mod = run_model(max_nodes)\n",
    "\n",
    "dif = max_nodes - min_nodes\n",
    "\n",
    "while dif > 5:\n",
    "\n",
    "    nodes = split(min_nodes, max_nodes)\n",
    "    dif = max_nodes - min_nodes\n",
    "    acc, mod = run_model(nodes)\n",
    "\n",
    "    if min_acc > acc and acc > max_acc:\n",
    "        max_nodes = nodes\n",
    "        max_acc = acc\n",
    "        max_mod = mod\n",
    "        best = min_mod\n",
    "\n",
    "    elif max_acc > acc and acc > min_acc:\n",
    "\n",
    "        min_nodes = nodes\n",
    "        min_acc = acc\n",
    "        min_mod = mod\n",
    "        best = max_mod\n",
    "    \n",
    "    elif acc > max_acc and acc > min_acc:\n",
    "\n",
    "        if max_acc > min_acc:\n",
    "            min_nodes = nodes\n",
    "            min_acc = acc\n",
    "            min_mod = mod\n",
    "            best = mod\n",
    "\n",
    "        elif min_acc > max_acc:\n",
    "            max_nodes = nodes\n",
    "            max_acc = acc\n",
    "            max_mod = mod\n",
    "            best = mod\n",
    "    print(f\"best model: {best.hidden_layer.node_count} nodes \")\n",
    "    best_acc = best.test_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layer Optimization Results\n",
    "\n",
    "I applied a binary search method to find the best number of nodes between 10 and 100 nodes.   \n",
    "The best model had 100 nodes in the hidden layer. The trend in the data above suggests that more nodes in the hidden layer leads to better predictons. This makes sense as more nodes means more parameters learned within the model allowing for more flexibility. It is very likely a model with more layers and nodes would result in better performance. However further optimization of the hidden layer in this model is out of scope"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_play",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
