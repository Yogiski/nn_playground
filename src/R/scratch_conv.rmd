---
title: Simple Convolutional Neural Network From Scratch
author: Chris Yogodzinski
output: pdf_document
---

# Notebook Overview
This notebook is a follow up to src/python/scratch_two_layer.ipynb.  
The previous notebook I made a simple neural network from scratch to predict handwritten digits in the mnist dataset.
The neural network consisted of one fully connected hidden layer and an output layer. 

In this notebook I will be adding a convolutional layer and pooling layer to this simple neural network.
The convolutions in this neural network function to key features from the dataset prior to the predictive fully connected layer. 

## Variables  
$I$     matrix: a mnist image of dim = ($m$ by $n$)      
$K$     matrix: kernel of dim = ($m_k$ by $n_k$)   
$s$     integer: stride, a constant set by user   
$\phi_{a}$ function: activation function of convolution         
$F$     matrix: a feature map of dim = ($m - m_k + 1$ by $n - n_k + 1$) or ($i$ by $j$)   
$\phi_{p}$ function: pooling function applied to feature map  
$P$     matrix: pooled feature map, condensed image of dim = ($\frac{m - m_k}{s}$ by $\frac{n - n_k}{s}$)

## Convolutional Layer
The convolutional layer in a cnn is named after a method known as kernel convolution.   
Kernel convolution works by passing a filter or kernal iteratively over the input matrix and transform the values of the input based on the values of the kernel.   
In this notebook our kernal transformation will be $\sum_{i}\sum_{j}I[m - i, n - j] \times K$.  
In words this is element-wise multiplication of the kernel-sized subset of $I$ and $K$ followed by the sum of the product.  
Kernel Convolution simplifies our image which reduces the number of parameters our model needs to learn in the fully connected layers.   
However the kernel itself is also a set of paramters that needs to be learned so that we're not losing important information when we simplify the images.  

## Importing the data
The following code chunks read in the binary formatted mnist images

```{R read-data}
setwd("~/Projects/nn_playground")

read_mnist <- function(label_path, im_path) {
    # read in labels
    f <- file(label_path, "rb")
    meta <- readBin(f, n = 2, "integer", endian = "big")
    labels <- as.integer(readBin(f, n = meta[2], "raw", endian = "big"))
    close(f)

    # read in imgs
    f <- file(im_path, "rb")
    meta <- readBin(f, n = 4, "integer", endian = "big")
    byte_count <- meta[2] * meta[3] * meta[4]
    imgs <- readBin(f, n = byte_count, "raw", endian = "big")
    close(f)
    imgs <- array(as.integer(imgs), dim = c(meta[3], meta[4], meta[2]))
    dat <- list(labels = labels, imgs = imgs) 
    return(dat)
}

dat_path = "data/mnist"
train_dat <- read_mnist(
  paste(dat_path, "train-labels.idx1-ubyte", sep = "/"),
  paste(dat_path, "train-images.idx3-ubyte", sep = "/")
)

test_dat <- read_mnist(
  paste(dat_path, "t10k-labels.idx1-ubyte", sep = "/"),
  paste(dat_path, "t10k-images.idx3-ubyte", sep = "/")
)
```
## Image Preprocessing
### Flipping The Images
The function above reads the images in upside down for some reason.   
In the code chunk below I flip the images right side up and visualize them.

```{R flip-images}
flip_all_imgs <- function(array) { 

    flip_image <- function(img) {
        flipped <- img[, ncol(img):1]
        return(flipped)
    }
    dims <- dim(array)
    flipped <- array(apply(array, 3, flip_image), dim = dims)
    return(flipped)
}

train_dat[["imgs"]] <- flip_all_imgs(train_dat[["imgs"]])
test_dat[["imgs"]] <- flip_all_imgs(test_dat[["imgs"]])

im_num <- 5 
# test image
img <- test_dat[["imgs"]][ , , im_num]
cat(test_dat[["labels"]][im_num])
image(img, col = gray((0:255) / 255))

# train image
img <- train_dat[["imgs"]][ , , im_num]
cat(train_dat[["labels"]][im_num])
image(img, col = gray((0:255) / 255))
```
### Adding padding to images
In this step I'm adding a border of empty pixels around the edge of the images.  
The purpose of this step is to allow our model to get a better "view" of the features around the edges of the image.
It is likely this is unnecessary for such simple images like the mnist dataset. In this step I am also scaling the pixel values to be between 0 and 1.
```{R add-padding-to-imgs}
# add zero value to pixels to edges of all images
add_padding_and_scale <- function(img) {
  top_bottom <- rep_len(0, ncol(img))
  img <- rbind(top_bottom, img, top_bottom)
  left_right <- rep_len(0, nrow(img))
  img <- unname(cbind(left_right, img, left_right))
  return(img)
}

train_dat[["imgs"]] <- array(
  apply(train_dat[["imgs"]], MARGIN = 3, add_padding_and_scale),
  dim = c(30, 30, 60000)
)

test_dat[["imgs"]] <- array(
  apply(test_dat[["imgs"]], MARGIN = 3, add_padding_and_scale),
  dim = c(30, 30, 10000)
)
```
## Forward Propagation
### Kernel Convlution
In the cell below I've written some functions to apply kernel convolution to an array of our images. 
You can see from the images printed at below the cell how some features of the digits have been filtered out after kernel convolution
```{R fp-kernel-convolution}


kernel_filt <- function(row_idx, col_idx, img, kernel, kernel_dim) {
  img_panel <- img[row_idx:(row_idx+kernel_dim[1] - 1), col_idx:(col_idx+kernel_dim[2] - 1)]
  filted_panel <- sum(img_panel * kernel)
  return(filted_panel)
}

kernel_conv <- function(img, kernel, kernel_dim, feat_map_idx, dim_out) {
  # wrapper function to use in Map
  apply_kernel_filter <- function(x) {
    kernel_filt(
      feat_map_idx[x, "Var1"],
      feat_map_idx[x, "Var2"],
      img,
      kernel,
      kernel_dim
    )
  }
  feat_map <- matrix( 
    sapply(rownames(feat_map_idx), FUN = apply_kernel_filter),
    nrow = dim_out,
    ncol = dim_out
  )
  #feat_map <- matrix( 
  #  unlist(Map(apply_kernel_filter, rownames(feat_map_idx))),
  #  nrow = dim_out,
  #  ncol = dim_out
  #)
  return(feat_map)
}

apply_kernel_conv <- function(imgs, kernel, kernel_dim, stride) {

  data_dim <- dim(imgs)
  # images are square so just need to make one index
  conv_idx <- seq(1, img_dim[2] - kernel_dim[2] + 1, by = stride)
  # generate two column matrix of all indeces needed for full convolution
  feat_map_idx <- expand.grid(rep(list(conv_idx), 2))
  dim_out <- ((img_dim[1] - kernel_dim[1]) / stride) + 1
  feat_maps <- apply(
    imgs,
    MARGIN = 3,
    function(x) kernel_conv(x, kernel, kernel_dim, feat_map_idx, dim_out)
  )
  feat_maps <- array(feat_maps, dim = c(dim_out, dim_out, data_dim[3]))
  return(feat_maps)
}


stride <- 1
kernel_dim <- c(10, 10)
kernel <- matrix(
  runif(prod(kernel_dim), -1, 1),
  nrow = kernel_dim[1],
  ncol = kernel_dim[2]
)
img_dim <- dim(train_dat[["imgs"]][, , 1])

# 6 by 6 kernel will take roughly 40 min to run on all 60k images without parallel processing
ti <- Sys.time()
feature_maps <- apply_kernel_conv(train_dat[["imgs"]][, , 1:2], kernel, kernel_dim, stride)
tf <- Sys.time()
print(tf - ti)

show_img <- 2
image(train_dat[["imgs"]][, , show_img], col = gray((0:255) / 255))
image(feature_maps[, , show_img], col = gray((0:255) / 255))

```
## Backward Propagation
```{R backward-propagation}
```
