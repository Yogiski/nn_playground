---
title: Simple Convolutional Neural Network From Scratch
author: Chris Yogodzinski
output: pdf_document
---

# Notebook Overview
This notebook is a follow up to src/python/scratch_two_layer.ipynb.  
The previous notebook I made a simple neural network from scratch to predict handwritten digits in the mnist dataset.
The neural network consisted of one fully connected hidden layer and an output layer. 

In this notebook I will be adding a convolutional layer and pooling layer to this simple neural network.
The convolutions in this neural network function to key features from the dataset prior to the predictive fully connected layer. 

## Variables  
$I$     matrix: a mnist image of dim = ($m$ by $n$)      
$K$     matrix: kernel of dim = ($m_k$ by $n_k$)   
$s$     integer: stride, a constant set by user   
$\phi_{a}$ function: activation function of convolution         
$F$     matrix: a feature map of dim = ($m - m_k + 1$ by $n - n_k + 1$) or ($i$ by $j$)   
$\phi_{p}$ function: pooling function applied to feature map  
$P$     matrix: pooled feature map, condensed image of dim = ($\frac{m - m_k}{s}$ by $\frac{n - n_k}{s}$)

## Convolutional Layer
The convolutional layer in a cnn is named after a method known as kernel convolution.   
Kernel convolution works by passing a filter or kernal iteratively over the input matrix and transform the values of the input based on the values of the kernel.   
In this notebook our kernal transformation will be $\sum_{i}\sum_{j}I[m - i, n - j] \times K$.  
In words this is element-wise multiplication of the kernel-sized subset of $I$ and $K$ followed by the sum of the product.

## Importing the data
The following code chunks read in the binary formatted mnist images

```{R read-data}
setwd("~/Projects/nn_playground")

read_mnist <- function(label_path, im_path) {
    # read in labels
    f <- file(label_path, "rb")
    meta <- readBin(f, n = 2, "integer", endian = "big")
    labels <- as.integer(readBin(f, n = meta[2], "raw", endian = "big"))
    close(f)

    # read in imgs
    f <- file(im_path, "rb")
    meta <- readBin(f, n = 4, "integer", endian = "big")
    byte_count <- meta[2] * meta[3] * meta[4]
    imgs <- readBin(f, n = byte_count, "raw", endian = "big")
    close(f)
    imgs <- array(as.integer(imgs), dim = c(meta[3], meta[4], meta[2]))
    dat <- list(labels = labels, imgs = imgs) 
    return(dat)
}

dat_path = "data/mnist"
train_dat <- read_mnist(
  paste(dat_path, "train-labels.idx1-ubyte", sep = "/"),
  paste(dat_path, "train-images.idx3-ubyte", sep = "/")
)

test_dat <- read_mnist(
  paste(dat_path, "t10k-labels.idx1-ubyte", sep = "/"),
  paste(dat_path, "t10k-images.idx3-ubyte", sep = "/")
)
```
### Flipping The Images
The function above reads the images in upside down for some reason.   
In the code chunk below I flip the images right side up and visualize them.

```{R flip-images}
flip_all_imgs <- function(array) { 

    flip_image <- function(img) {
        flipped <- img[, ncol(img):1]
        return(flipped)
    }
    dims <- dim(array)
    flipped <- array(apply(array, 3, flip_image), dim = dims)
    return(flipped)
}

train_dat[["imgs"]] <- flip_all_imgs(train_dat[["imgs"]])
test_dat[["imgs"]] <- flip_all_imgs(test_dat[["imgs"]])

im_num <- 5 
# test image
img <- test_dat[["imgs"]][ , , im_num]
cat(test_dat[["labels"]][im_num])
image(img, col = gray((0:255) / 255))

# train image
img <- train_dat[["imgs"]][ , , im_num]
cat(train_dat[["labels"]][im_num])
image(img, col = gray((0:255) / 255))
```
### Adding padding to images
In this step I'm adding a border of empty pixels around the edge of the images.  
The purpose of this step is to allow our model to get a better "view" of the features around the edges of the image.
It is likely this is unnecessary for such simple images like the mnist dataset.  
```{R add-padding-to-imgs}
# add zero value to pixels to edges of all images
add_padding <- function(img) {
  top_bottom <- rep_len(0, ncol(img))
  img <- rbind(top_bottom, img, top_bottom)
  left_right <- rep_len(0, nrow(img))
  img <- unname(cbind(left_right, img, left_right))
  return(img)
}

train_dat[["imgs"]] <- array(
  apply(train_dat[["imgs"]], MARGIN = 3, add_boundary),
  dim = c(30, 30, 60000)
)

test_dat[["imgs"]] <- array(
  apply(test_dat[["imgs"]], MARGIN = 3, add_boundary),
  dim = c(30, 30, 10000)
)
```
## Forward Propagation
```{R forward propagation}
forward_propagation <- function()

kernel_conv <- function(img, img_dim, kernel, kernel_dim, stride) {
  # images are square so just need to make one index
  conv_idx <- seq(1, img_dim[2] - kernel_dim[2], by = stride)

   # using random kernel element values either 0 or 1
  subs_img <- img[j:j+kernel_dim[1], i:i+kernel_dim[2]]
  filted_subs <- sum(subs_img * kernel)

  return(filted_img)
}

stride <- 2
kernel_dim <- c(3 , 3)
kernel <- matrix(

  data = rbinom(prod(kernel_dim), 1, 0.5),
  nrow = kernel_dim[1],
  ncol = kernel_dim[2]
)
img_dim <- dim(train_dat[["imgs"]][, , 1])

kernel_conv(img, img_dim, kernel, kernel_dim, stride)

```
## Backward Propagation
```{R backward-propagation}
```
