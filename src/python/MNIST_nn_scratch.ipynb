{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Classifier Using MNIST #\n",
    "\n",
    "MNIST is a dataset of low res images of hand written digits.\n",
    "In this notebook I will explore the math and implement from scratch in python different nueral network structures and compare their performance.\n",
    " \n",
    "Data Structure and Notation:\n",
    "* 28 x 28 pixel images, 784 pixels\n",
    "* $m$ training images\n",
    "* $X$ is a matrix with dimensions $m$ by 784 \n",
    "    * $X^{T}$ is used for input into nueral network model\n",
    "\n",
    " ## Basic NN Structure ##\n",
    "input layer: 784 nodes \\\n",
    "hidden layer: 10 nodes \\\n",
    "output layer: 10 nodes for 10 classes or digits \n",
    "\n",
    "## Forward Propagation ##\n",
    "Forward propagation is the process of transforming our input data into our output data.\n",
    "\n",
    "The input layer is simply a matrix of our raw training data. \\\n",
    "Input Layer Definition:\n",
    "* $A^{[0]} = X\\space\\space\\text{with dimensions } 784\\space \\text{x} \\space m$\n",
    "\n",
    "The hidden layer values are obtained by a linear transformation of the input layer followed by a non-linear transformation or activation of the transformed input data. \\\n",
    "We are using the rectified linear unit or ReLU as the activation function for our hidden layer. \n",
    "\n",
    "$\\text{RelU}(x) = \\{x \\hspace{0.5em} \\text{if} \\hspace{0.5em} x > 0, \\hspace{1em} 0 \\hspace{0.5em} \\text{if} \\hspace{0.5em} \\le 0\\}$ \n",
    "\n",
    "Hidden Layer Definition:\n",
    "* $Z^{[1]} = w^{[1]} \\cdot A^{[0]} + b^{[1]}$ (Unactivated: linear transformation of input layer)\n",
    "* $A^{[1]} = g(Z^{[1]}) = \\text{ReLu}(Z^{[1]})$ (Activated: ReLU transformation of linear transformation)\n",
    "\n",
    "The output layer works similarly to the hidden layer but with an output specific activation function. \\\n",
    "The hidden layer is linearally transformed and this time we use a softmax as an activation function to convert our transformed values into probabilities. \n",
    "The softmax essentially calculates the contribution of each not to the total output vector.\n",
    "\n",
    "$\\text{softmax}(x) = \\dfrac{e^{x_{i}}}{\\sum_{i=1}^{k} e^{x_{i}}}$\n",
    "\n",
    "* $Z^{[2]} = w^{[2]} \\cdot A^{[2]} + b^{[2]}$ (Unactivated: linear tranformation of hidden layer)\n",
    "* $A^{[2]} = \\text{softmax}(Z^{[2]})$ (Activated: probabilities of input being a given class)\n",
    "\n",
    "## Back Propagation ##\n",
    "Back Propagation is the process we use to learn the weights and biases in the forward propagation process. \\\n",
    "It works by finding the error in the weights, biases, and predictions working backward from the output layer. \n",
    "\n",
    "First step is to find the error of the output layer by subtracting the classes from our output layer matrix \\\n",
    "Y is the digit pictured in each image in the training set. Y is a one-hot encoded 10 by m matrix. \n",
    "\n",
    "* $dZ^{[2]} = A^{[2]} - Y$ (error of predictions)\n",
    "\n",
    "The next step is to find the error associated with weights and biases used in the linear transformation in the second layer.\n",
    "\n",
    "* $dw^{[2]} = \\frac{1}{m} dZ^{[2]} \\cdot A^{[1]T}$ (contributions of layer 2 weights to prediction error)\n",
    "* $db^{[2]} = \\frac{1}{m} \\sum dZ^{[2]}$ (contributions of layer 2 biases to prediction error)\n",
    "\n",
    "Repeat this process to find error of weights and biases in the first layer. Must undo activation function \\\n",
    "$g`$ is the derivative of the activation function\n",
    "* $dZ^{[1]} = w^{[2]T} dZ^{[2]} \\cdot g`(Z^{[1]})$ (error of unactivated layer 2 nodes)\n",
    "* $dw^{[1]} = \\frac{1}{m} dZ^{[1]} X^T$ (contribution of weights to layer 2 error)\n",
    "* $db^{[1]} = \\frac{1}{m} \\sum dZ^{[1]}$ (contribution of biases to layer 2 error)\n",
    "\n",
    "## Weight and Bias Update ##\n",
    "Weights and biases are then updated after calculating their errors in backpropagation. \\\n",
    "The new values are generated by subtracting the error multiplied by a learning rate parameter. \\\n",
    "The learning rate $\\alpha$ is a user defined constant that modifies how much the weights and biases are changed in each cycle\n",
    "\n",
    "* $w^{[1_{}]}_{new} = w^{[1]} - \\alpha dw^{[1]}$\n",
    "* $b^{[1]}_{new} = b^{[1]} - \\alpha db^{[1]}$\n",
    "* $w^{[2]}_{new} = w^{[2]} - \\alpha dw^{[1]}$\n",
    "* $b^{[2]}_{new} = b^{[2]} - \\alpha db^{[2]}$\n",
    "\n",
    "This process of using forward propagation to make predictions follow by backward propagation to find the error and \"learn\" new weights \\\n",
    "is repeats till our predictions reach reach some sort of convergence. This training process in known as gradient descent. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2051, 60000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1695acd90>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "datpath = \"../../data/mnist\"\n",
    "image_size = 28\n",
    "num_images = 10\n",
    "\n",
    "# open training data which is \n",
    "f = open(f\"{datpath}/train-images-idx3-ubyte/train-images-idx3-ubyte\", mode = \"rb\")\n",
    "# first 16 bytes hold training image metadata\n",
    "print(struct.unpack(\">IIII\", f.read(16)))\n",
    "buf = f.read(image_size * image_size * num_images)\n",
    "data = np.frombuffer(buf, dtype = np.uint8).astype(np.float32)\n",
    "data = data.reshape(num_images, image_size, image_size, 1)\n",
    "image = np.asarray(data[5]).squeeze()\n",
    "plt.imshow(image, cmap = plt.cm.gray_r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08807971, 0.08807971, 0.08807971, 0.08807971, 0.08807971,\n",
       "       0.08807971, 0.08807971, 0.08807971, 0.08807971, 0.08807971,\n",
       "       0.01192029, 0.01192029, 0.01192029, 0.01192029, 0.01192029,\n",
       "       0.01192029, 0.01192029, 0.01192029, 0.01192029, 0.01192029])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### ACTIVATION FUNCTIONS ####\n",
    "def ReLU(vec: np.array):\n",
    "    return np.maximum(0, vec)\n",
    "\n",
    "def deriv_ReLU(vec: np.array):\n",
    "    return vec > 0\n",
    "\n",
    "def softmax(vec: np.array):\n",
    "    return np.exp(vec) / np.sum(np.exp(vec))\n",
    "\n",
    "\n",
    "\n",
    "##### NN CLASSES #######\n",
    "\n",
    "class Layer(object):\n",
    "\n",
    "    def __init__(self, layer_id, input_size, node_count, activation_func, layer_type = \"hidden\"):\n",
    "\n",
    "        # Layer metadata\n",
    "        self.layer_id = layer_id\n",
    "        self.node_count = node_count\n",
    "        self.layer_type = layer_type\n",
    "        self.activation_func = activation_func\n",
    "        # random init of weights and biases\n",
    "        self.weights = np.random.rand(size = (node_count, input_size))\n",
    "        self.biases = np.random.rand(size = (node_count, 1))\n",
    "    \n",
    "    def transform(self, input_dat: np.array):\n",
    "\n",
    "        lin_transform = self.weights.dot(input_dat) + self.biases\n",
    "        self.unactivated_node_values = lin_transform\n",
    "        self.node_values = self.acivation_func(lin_transform) \n",
    "\n",
    "        return self.node_values\n",
    "\n",
    "    def update_weights_n_biases(self, weight_error, bias_error, learning_rate):\n",
    "        pass\n",
    "\n",
    "\n",
    "class TwoLayerNN(object):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            train_dat: np.array,\n",
    "            train_labels: np.array,\n",
    "            test_dat: np.array,\n",
    "            test_labels: np.array,\n",
    "            hidden_nodes: int,\n",
    "            output_nodes: int,\n",
    "            learning_rate: float,\n",
    "        ):\n",
    "\n",
    "        self.train_dat = train_dat\n",
    "        self.train_labels = train_labels\n",
    "        self.test_dat = test_dat\n",
    "        self.test_labels = test_labels\n",
    "        self.learning_rate = learning_rate\n",
    "        # layer init\n",
    "        self.hidden_layer = Layer(\"hidden\", hidden_nodes, self.traind_dat.shape[0], activation_func = ReLU)\n",
    "        self.output_layer = Layer(\"out\", output_nodes, hidden_nodes, activation_func = softmax, layer_type = \"out\")\n",
    "\n",
    "    def forward_prop(self):\n",
    "        hidden_transformed = self.hidden_layer.transfrom(self.train_dat)\n",
    "        out = self.output_layer.transform(hidden_transformed)\n",
    "        self.out = out\n",
    "\n",
    "    def backward_prop(self):\n",
    "\n",
    "        m = self.train_dat.shape[1]\n",
    "        # prediction error\n",
    "        dZ_o = self.out - \"one_hot(self.train_labels)\"\n",
    "        dw_o = (1 / m) * dZ_o.dot(self.hidden_layer.node_values.T) \n",
    "        db_o =  np.sum(dZ_o, 2) / m\n",
    "        # hidden layer error\n",
    "        dZ_h =  self.output_layer.weights.T.dot(dZ_o) * deriv_ReLU(self.hidden_layer.unactivated_node_values)\n",
    "        dw_h = (1 / m) * dZ_h.dot(self.train.T)\n",
    "        db_h =  np.sum(dZ_h) / m\n",
    "        # update weights and biases from error components\n",
    "        self.hidden_layer.update_weights_n_biases(dw_h, db_h, self.learning_rate)\n",
    "        self.output_layer.update_weights_n_biases(dw_o, db_o, self.learning_rate)\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def test(self):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 28, 28, 1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7840,)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.flatten().shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_play",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
